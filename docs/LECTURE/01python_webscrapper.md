---
title: ğŸˆ Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í¼ ë§Œë“¤ê¸° (nomad coders/ë…¸ë§ˆë“œì½”ë”)
meta:
  - name: description
    content: python nomadCoders ë…¸ë§ˆë“œì½”ë”
  - name: keywords
    content: python nomadCoders ë…¸ë§ˆë“œì½”ë”
---

# ğŸˆ Pythonìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í¼ ë§Œë“¤ê¸° (nomad coders/ë…¸ë§ˆë“œì½”ë”)

ì‹¬ì‹¬í•´ì„œ ë“¤ì–´ë³´ëŠ” Python ê°•ì˜ .. ğŸ‘“ 

2022-04 ê°•ì˜ë¥¼ ë“¤ì—ˆì„ ë•Œ, indeed ì‚¬ì´íŠ¸ ë§ˆí¬ì—… êµ¬ì¡°ê°€ ì¢€ ë‹¬ë¼ì¡Œê³ , stackoverflow job ì‚¬ì´íŠ¸ëŠ” ì°¾ì„ ìˆ˜ ì—†ì—ˆë‹¤,, ëŒ€ì‹  `https://stackoverflow.com/jobs/companies` ì‚¬ì´íŠ¸ë¥¼ íŒŒì‹±í•´ë³´ë ¤ê³  í•œë‹¤.

<u>ë”°ë¼ì„œ ê°•ì˜ì— ì‘ì„±ëœ ì½”ë“œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ!</u> 

ìš°ì„  ê°•ì˜ë¥¼ ë“£ê³ , ë°”ë€ HTMLì„ ë°°ìš´ëŒ€ë¡œ ìŠ¤í¬ë©í•‘ í•˜ì˜€ë‹¤. ê°•ì˜ ì†Œì œëª© ë½‘ìœ¼ë ¤ê³  ìŠ¤í¬ë©í•‘ í™œìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì œëª©ì„ ë½‘ì•˜ë‹¤ !

`beautiful soup` ìœ ìš©í•œ ê±° ê°™ë‹¤. ì˜ ì¨ë´ì•¼ì§€.

<h3>ğŸ”¸ py</h3>

```py
import requests
from bs4 import BeautifulSoup

result = requests.get("https://nomadcoders.co/python-for-beginners/lobby")
soup = BeautifulSoup(result.text, 'html.parser')
titles = soup.find_all("span", {"class": "px-6 py-4 whitespace-nowrap text-sm leading-5 overflow-hidden font-medium flex items-center text-gray-400"})
for title in titles:
  print(title.text)
```

<h3>ğŸ”¹ console</h3>

```html
#0.5 How to Ask for Help (02:00) 
#0.6 Code Python Online (03:08) 
#1.0 Data Types of Python (08:48) 
#1.1 Lists in Python (08:30) 
#1.2 Tuples and Dicts (06:33) 
...
ì¤‘ëµ
...
#4.6 Rendering Jobs! (12:24) 
#4.7 Export Route (08:48) 
#4.8 File Download (05:21) 
#4.9 Recap (07:28) 
#4.10 Conclusions (02:56) 
```
# âš¡ 1. THEORY

[repl.it](https://replit.com/) ì—ì„œ ê°„ë‹¨í•˜ê²Œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë‹¤.

## 1.0 Data Types of Python

<h3>ğŸ”¸ py</h3>

```py
# python ë³€ìˆ˜ëª…ì€  snakeCaseë¡œ ì¨ì¤€ë‹¤.
a_string = 'like this'
a_number = 3
a_float = 3.12
a_boolean = False # pythonì—ì„œëŠ” ì²«ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ ì¨ì•¼í•œë‹¤.
a_none = None # empty ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.

# ì¶œë ¥
print(type(a_number))
print(type(a_string))
print(type(a_none))
```

<h3>ğŸ”¹ console</h3>

```md
<class 'int'>
<class 'str'>
<class 'NoneType'>
```

## 1.1 Lists in Python

<h3>ğŸ”¸ py</h3>

```py
# mutable sequence (ë³€ê²½ ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤)
days = ["Mon", "tue", "Wed", "Thur", "Fri"]

print("------list------")
print(days)
print("Mon" in days)
print("Man" in days)
print(days[3])
print(type(days))

print("------append------")
days.append("Sat")
print(days)

print("------reverse------")
days.reverse()
print(days)
```

<h3>ğŸ”¹ console</h3>

```md
------list------
['Mon', 'tue', 'Wed', 'Thur', 'Fri']
True
False
Thur
<class 'list'>
------append------
['Mon', 'tue', 'Wed', 'Thur', 'Fri', 'Sat']
------reverse------
['Sat', 'Fri', 'Thur', 'Wed', 'tue', 'Mon']
```

## 1.2 Tuples and Dicts

<h3>ğŸ”¸ py</h3>

```py
# immutable sequence (ë³€ê²½ ë¶ˆê°€ëŠ¥í•œ ì‹œí€€ìŠ¤)
days = ("Mon", "tue", "Wed", "Thur", "Fri")

print("------tuple------")
print(days)
print(type(days))

print("------dictionary------")
nico = {
  "name": "Nico",
  "age": 29,
  "korean": True,
  "fav_food": ["Kimchi", "Sashimi"]
}

print(type(nico))
print(nico)
nico["handsome"] = True
print(nico)
```

<h3>ğŸ”¹ console</h3>

```md
------tuple------
('Mon', 'tue', 'Wed', 'Thur', 'Fri')
<class 'tuple'>
------dictionary------
<class 'dict'>
{'name': 'Nico', 'age': 29, 'korean': True, 'fav_food': ['Kimchi', 'Sashimi']}
{'name': 'Nico', 'age': 29, 'korean': True, 'fav_food': ['Kimchi', 'Sashimi'], 'handsome': True}
```

## 1.3 Built in Functions

[ë‹¤ì–‘í•œ Functions](https://docs.python.org/3/library/functions.html)

<h3>ğŸ”¸ py</h3>

```py
print("------len------")
print(len("fjsiodjfoisjf"))

print("------type------")
age = "18"
print(type(age))
n_age = int(age)
print(type(n_age))
```

<h3>ğŸ”¹ console</h3>

```md
------len------
13
------type------
<class 'str'>
<class 'int'>
```

## 1.4 Creating a Your First Python Function

<h3>ğŸ”¸ py</h3>

```py
# ë“¤ì—¬ì“°ê¸°ë¥¼ í•´ì¤˜ì•¼ defineë¨ !
# pythonì€ {}ìœ¼ë¡œ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤.
def say_hello(): 
  print("hello")
  print("bye")

say_hello()
say_hello()
say_hello()
```

<h3>ğŸ”¹ console</h3>

```md
hello
bye
hello
bye
hello
bye
```

## 1.5 Function Arguments

<h3>ğŸ”¸ py</h3>

```py
def say_hello(who): 
  print("hello", who)

say_hello("Nico")
say_hello(True)
# say_hello() - error!

print("------")

def plus(a, b):
  print(a + b)
def minus(a, b = 0): # default value
  print(a - b)
  
plus(2, 5)
minus(2)
minus(2, 5)
```

<h3>ğŸ”¹ console</h3>

```md
hello Nico
hello True
------
7
2
-3
```

## 1.6 Returns

<h3>ğŸ”¸ py</h3>

```py
def p_plus(a, b):
  print(a + b)
  
def r_plus(a, b):
  return a + b
  print("test") # return ì´í›„ë¡œ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

p_result = p_plus(2, 3)
r_result = r_plus(2, 3)

print(p_result, r_result)
```

<h3>ğŸ”¹ console</h3>

```md
5
None 5
```

## 1.7 Keyworded Arguments

<h3>ğŸ”¸ py</h3>

```py
# format
def say_hello(name, age):
  return f"Hello {name} you are {age} years old"

 # ì¸ìê°€ ë°”ë€Œì–´ë„ ê´œì°®ìŒ! ìˆœì„œê°€ ìƒê´€ì—†ë‹¤!
hello = say_hello(age = "12", name = "Nico")
print(hello)
```

<h3>ğŸ”¹ console</h3>

```md
Hello Nico you are 12 years old
```

## 1.8 Code Challenge!

7ê°œ ì—°ì‚° ê³„ì‚°ê¸° ë§Œë“¤ê¸°

<h3>ğŸ”¸ py</h3>

```py
# 7ê°œ ì—°ì‚°
def plus(a, b):
  return calc(a, b, '+')

def minus(a, b):
  return calc(a, b, '-')

def times(a, b):
  return calc(a, b, '*')

def division(a, b):
  return calc(a, b, '/')

def nega(a, b):
  return calc(a, b, 'nega')

def remain(a, b):
  return calc(a, b, '%')

def power(a, b):
  return calc(a, b, '**')
  
# validate and calcuate
def calc(a, b, func):
  try:
    a = float(a)
    b = float(b)
    try:
      if func == '+':
        return a + b
      elif func == '-':
        return a - b
      elif func == '*':
        return a * b
      elif func == '/':
        return a / b
      elif func == 'nega':
        return -a
      elif func == '**':
        return a ** b
    except:
      return 'Please check a method name.'
  except:
    return 'Please enter a number.'

print("------print(plus(3,'test'))------")
print(plus(3,'test'))
print("------print(minus(3,5))------")
print(minus(3,5))
```

<h3>ğŸ”¹ console</h3>

```md
------print(plus(3,'test'))------
Please enter a number.

------print(minus(3,5))------
-2.0
```

## 1.9 Conditionals Part One

<h3>ğŸ”¸ py</h3>

```py
def plus(a, b):
  if type(b) is int or type(b) is float:
    return a + b
  else:
    return None

print(plus(12, 1.2))
print(plus(12, "test"))
```

<h3>ğŸ”¹ console</h3>

```md
13.2
None
```

## 1.10 if else and of

<h3>ğŸ”¸ py</h3>

```py
def age_check(age):
  print(f"------you are {age}------")
  if age < 18:
    print("you cant drink")
  elif age == 18:
    print("you are new to this!")
  elif age > 20 and age < 25:
    print("you are still kind of young")
  else:
    print("enjoy your drink")
  print()

age_check(16)
age_check(18)
age_check(23)
age_check(30)
```

<h3>ğŸ”¹ console</h3>

```md
------you are 16------
you cant drink

------you are 18------
you are new to this!

------you are 23------
you are still kind of young

------you are 30------
enjoy your drink
```

## 1.11 for in

<h3>ğŸ”¸ py</h3>

```py
days = ("Mon", "Tue", "Wed", "Thur", "Fri")

for d in days:
  if d == 'Wed':
    break
  else:
    print(d)

for n in [1, 2, 3, 4, 5]:
  print(n)

for letter in "nicolas":
  print(letter)
```

<h3>ğŸ”¹ console</h3>

```md
Mon
Tue
1
2
3
4
5
n
i
c
o
l
a
s
```

## 1.12 Modules

<h3>ğŸ”¸ main.py</h3>

```py
# í•­ìƒ ì‚¬ìš©í•  ê²ƒë§Œ ê°€ì ¸ì˜¤ë„ë¡ !
from math import ceil, fsum as sexy_sum
from calculator import plus 

print(ceil(1.2))
print(sexy_sum([1, 2, 3, 4, 5, 6, 7]))
print(plus(2, 3))
```

<h3>ğŸ”¸ calcuator.py</h3>

```py
def plus(a, b):
  return a + b
```

<h3>ğŸ”¹ console</h3>

```md
2
28.0
5
```
# âš¡ 2. BUILDING A JOB SCRAPPER

## 2.0~1 What is Web Scrapping~What are We Building

íŒŒì´ì¬ìœ¼ë¡œ ì‚¬ì´íŠ¸ ìŠ¤í¬ë©í•‘í•˜ëŠ” ê¸°ëŠ¥ì„ ë§Œë“¤ ì˜ˆì • -

indeed í•œêµ­ ë²„ì „ì´ ìƒê²¨ì„œ ì¢€ í—·ê°ˆë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ `https://www.indeed.com/jobs?q=python&limit=50` ë°”ë¡œ ì´ urlë¡œ ìŠ¤í¬ë©í•‘ í•˜ë„ë¡ í•  ê²ƒ !

stackOverflow jobì€ ì‚¬ë¼ì§„ ê±° ê°™ë‹¤,, ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë¡œ ëŒ€ì²´í•˜ë˜ì§€ í•´ì„œ í…ŒìŠ¤íŠ¸ í•´ë³´ì.

## 2.2 Navigating with Python

<h3> ğŸ“Œ ìš°ì„  HTML ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì.</h3>

:::tip python requests
`https://docs.python-requests.org/en/latest/` í™œìš©í•˜ì—¬ Http Requests ì°Œë¥´ê¸°

`repl.it`ì—ì„œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ í•´ì£¼ì. requestsë¥¼ ê²€ìƒ‰í•˜ì—¬ `Python HTTP for Humans` ì„¤ì¹˜

```py
r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
r.status_code
200
r.headers['content-type']
'application/json; charset=utf8'
r.encoding
'utf-8'
r.text
'{"type":"User"...'
r.json()
{'private_gists': 419, 'total_private_repos': 77, ...}
```

:::

<h3>ğŸ”¸ py</h3>

```py 
import requests

indeed_result = requests.get("https://www.indeed.com/jobs?q=python&limit=50")

print(indeed_result.text)
```

<h3>ğŸ”¹ console</h3>

html í…ìŠ¤íŠ¸ê°€ ì¶œë ¥ëœë‹¤.

```md
.
.
.
k\u0004My new jobs":[null,""],"job feed link\u0004There are new jobs":[null,""],"jobresults_tip_l_empty_variation_1\u0004Tip: Enter your city or zip code in the \"where\" box to show results in your area.":[null,"Tip: Enter your city or zip code in the \"where\" box to show results in your area."],"mobile_home_query_caption\u0004Job title, keywords, or company":[null,""],"near {0}":[null,""],"new count\u0004{0} new":[null,""],"new count / location separator\u0004in {0}":[null,""],"notice_message_for_empty_q_and_l\u0004Enter a job title or location to start a search":[null,""],"pill_filters\u0004All jobs":[null,""],"pill_filters\u0004Date Posted":[null,""],"pill_filters\u0004Last 14 days":[null,""],"pill_filters\u0004Last 24 hours":[null,""],"pill_filters\u0004Last 3 days":[null,""],"pill_filters\u0004Last 7 days":[null,""],"radius-slider-title\u0004Distance":[null,""],"recent search\u0004There is no recent search":[null,""],"recent search\u0004There is no recent search history.":[null,""],"recent search item\u0004go":[null,""],"recent search item\u0004{0} - {1}":[null,""],"recent_search_aria_label\u0004hide":[null,""],"recent_search_aria_label\u0004please tap the bottom of this page for back to search result.":[null,""],"recent_search_aria_label\u0004show":[null,""],"recent_search_ssr_label\u0004edit searches":[null,""],"recent_search_ssr_label\u0004finish":[null,""],"recent_searches_heading\u0004Search history / Saved Searches":[null,""],"rich search\u0004add filters":[null,""],"rich search\u0004dismiss":[null,""],"search":[null,""],"single search\u0004change query":[null,""],"{0} miles":[null,""],"{0} search suggestion":["{0} search suggestions","",""]};}).bind(this.mosaic.i18nOverrides)();
</script>
<script>window['sendPageLoadEndPing'] = function(pageId, tk, st) {var validPageIds = ['viewjob', 'serp']; if (!!Image && validPageIds.indexOf(pageId) > -1 && !!tk && !!st) {var href = '/rpc/pageLoadEnd?pageId=' + pageId + '&tk=' + tk + '&st=' + st + '&__=' + Math.random(); var img = new Image(); img.src = href;}}; window['sendPageLoadEndPing']("serp", "1g17eob8vghqc800", "1650591542559");</script><div class="mosaic-zone" id="mosaic-zone-serpBottomBody"><div id="mosaic-provider-signinprompt" class="mosaic mosaic-provider-signinprompt mosaic-rst"></div><div id="mosaic-provider-dislike-feedback" class="mosaic mosaic-provider-dislike-feedback"><div class="animatedToast i-unmask"><div class=""></div></div></div></div><script type="text/javascript">
                try {
                    window.mosaic.onMosaicApiReady(function() {
                        var zoneId = 'serpBottomBody';
                        var providers = window.mosaic.zonedProviders[zoneId];

                        if (providers) {
                            providers.filter(function(p) { return window.mosaic.lazyFns[p]; }).forEach(function(p) {
                                return window.mosaic.api.loadProvider(p);
                            });
                        }
                    });
                 } catch (e) {};
                </script></body>
</html>
```
<h3> ğŸ“Œ <u>beautiful soup</u> ì„ í™œìš©í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë§Œ ê°€ì ¸ì˜¤ì</h3>

:::tip BeautifulSoup
`https://www.crummy.com/software/BeautifulSoup/bs4/doc/`

`repl.it`ì—ì„œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ í•´ì£¼ì. `beautifulsoup4` ê²€ìƒ‰í•˜ì—¬ ì„¤ì¹˜

```py
from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')

print(soup.prettify())
```

```py
soup.title
# <title>The Dormouse's story</title>

soup.title.name
# u'title'

soup.title.string
# u'The Dormouse's story'

soup.title.parent.name
# u'head'

soup.p
# <p class="title"><b>The Dormouse's story</b></p>

soup.p['class']
# u'title'

soup.a
# <a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>

soup.find_all('a')
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]

soup.find(id="link3")
# <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>
```
:::

## 2.3 Extracting Indeed Pages

í˜ì´ì§• ë²ˆí˜¸ê°€ `<div class="pagination">...`ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. í•´ë‹¹ ë‚´ìš© íŒŒì‹±í•´ë³´ì.

<h3>ğŸ”¸ py</h3>

```py
import requests
from bs4 import BeautifulSoup

indeed_result = requests.get("https://www.indeed.com/jobs?q=python&limit=50")

indeed_soup = BeautifulSoup(indeed_result.text, 'html.parser')

# ì „ì²´ ì¶œë ¥
# print(indeed_soup)

# <title>
# print(indeed_soup.title)

# í˜ì´ì§• div ì°¾ê¸°
pagination = indeed_soup.find("div", {"class": "pagination"})

# a link ì°¾ê¸°
pages = pagination.find_all('a')

# <span class="pn">ë²ˆí˜¸</span> ì°¾ê¸°

spans = []

for page in pages:
  spans.append(page.find("span"))

print("---------span/class:pn---------")
print(spans)
print("---------Listì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ item---------")
 # -1: ë§ˆì§€ë§‰ì—ì„œë¶€í„° ì‹œì‘í•´ì„œ ì²« item
print(spans[-1])
print("---------Listì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ item ë¹¼ê³  ì¡°íšŒ---------")
# ë§ˆì§€ë§‰ ì•„ì´í…œ ë¹¼ê³  ì¡°íšŒ
# spans[0:5] 0ë¶€í„° 5ê°œ ì¡°íšŒ
print(spans[:-1])

```

<h3>ğŸ”¹ console</h3>

```md
---------span/class:pn---------
[<span class="pn">2</span>, <span class="pn">3</span>, <span class="pn">4</span>, <span class="pn">5</span>, <span class="pn"><span class="np"><svg fill="none" height="24" width="24"><path d="M10 6L8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6-6-6z" fill="#2D2D2D"></path></svg></span></span>]
---------Listì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ item---------
<span class="pn"><span class="np"><svg fill="none" height="24" width="24"><path d="M10 6L8.59 7.41 13.17 12l-4.58 4.59L10 18l6-6-6-6z" fill="#2D2D2D"></path></svg></span></span>
---------Listì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ item ë¹¼ê³  ì¡°íšŒ---------
[<span class="pn">2</span>, <span class="pn">3</span>, <span class="pn">4</span>, <span class="pn">5</span>]
```

## 2.4 Extracting Indeed Pages part Two

`pages.append(link.("span").string)` ëŠ” `pages.append(link.string)` ì™€ ê°™ë‹¤.
**beautiful soup** ì—ì„œ link > span ì•ˆì— stringì„ ì½ì–´ì¤€ë‹¤.

<h3>ğŸ”¸ py</h3>

```py
import requests
from bs4 import BeautifulSoup

indeed_result = requests.get("https://www.indeed.com/jobs?q=python&limit=50")

indeed_soup = BeautifulSoup(indeed_result.text, 'html.parser')

# í˜ì´ì§• div ì°¾ê¸°
pagination = indeed_soup.find("div", {"class": "pagination"})

# a link ì°¾ê¸°
links = pagination.find_all('a')

# <span class="pn">ë²ˆí˜¸</span> ì°¾ê¸°
pages = []
for link in links[:-1]:
  #pages.append(link.("span").string) ì™€ ê°™ìŒ.
  # intë¡œ cast
  pages.append(int(link.string))

# htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€
max_page = pages[-1]
print(max_page)
```

<h3>ğŸ”¹ console</h3>

```md
5
```

## 2.5 Requesting Each Page

<h3>ğŸ“Œ ëª¨ë“  í˜ì´ì§€ë¥¼ requestí•´ë³´ì.</h3>

íŒŒì¼ì„ ë”°ë¡œ ë‘ì–´ functionì„ ì •ë¦¬í•´ì¤¬ë‹¤.

<h3>ğŸ”¸ main.py</h3>

```py
from indeed import extract_indeed_pages, extract_indeed_jobs

# page ë²ˆí˜¸ ì „ë¶€ ì¡°íšŒ í›„,
last_indeed_page = extract_indeed_pages()

# request ë‚ ë ¤ë³´ê¸° status 200 ì„±ê³µ ì¶œë ¥
extract_indeed_jobs(last_indeed_page)
```

<h3>ğŸ”¸ indeed.py</h3>

```py
import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

# pagination ì¤‘ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸
def extract_indeed_pages():
  result = requests.get(URL)
  
  soup = BeautifulSoup(result.text, 'html.parser')
  
  # htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
  pagination = soup.find("div", {"class": "pagination"})
  
  links = pagination.find_all('a')
  
  pages = []
  for link in links[:-1]:
    pages.append(int(link.string))
  
  max_page = pages[-1]
  return max_page

# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
#  range(N): Nê°œì˜ ë°°ì—´ì„ ìƒì„±í•´ì¤Œ.
def extract_indeed_jobs(last_page):
  for page in range(last_page):
    result = requests.get(f"{URL}&start={page*LIMIT}")
    print(result.status_code)
```

<h3>ğŸ”¹ console</h3>

```md
200
200
200
200
200
```

## 2.6 Extracting Titles

ì‚¬ì´íŠ¸ ë§ˆí¬ì—…ì´ ì¢€ ë‹¬ë¼ì ¸ì„œ ê°•ì˜ë¥¼ ë“£ê³ , ë‚´ ë°©ì‹ëŒ€ë¡œ ìˆ˜ì •í•˜ì—¬ titleì„ ê°€ì ¸ì™”ë‹¤.

ì—¬ê¸°ì„œë¶€í„° ê°•ì˜ì™€ ì½”ë“œê°€ ì¢€ ë‹¤ë¥¼ ì˜ˆì •,,

<h3>ğŸ”¸ py</h3>

```py
import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

# pagination ì¤‘ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸
def extract_indeed_pages():
  result = requests.get(URL)
  
  soup = BeautifulSoup(result.text, 'html.parser')
  
  # htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
  pagination = soup.find("div", {"class": "pagination"})
  
  links = pagination.find_all('a')
  
  pages = []
  for link in links[:-1]:
    pages.append(int(link.string))
  
  max_page = pages[-1]
  return max_page

# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
def extract_indeed_jobs(last_page):
  jobs = []
  # 1í˜ì´ì§€ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ forë¬¸ ì£¼ì„ ì²˜ë¦¬
  #for page in range(last_page):
  result = requests.get(f"{URL}&start={0*LIMIT}")
  soup = BeautifulSoup(result.text, 'html.parser')
  results = soup.find_all("div", {"class": "job_seen_beacon"})
  for result in results:
    title = result.find("td", {"class": "resultContent"}).find("h2", {"class": "jobTitle"}).find("span", {"class": None})["title"]
    print(title) # title
  return jobs
```

<h3>ğŸ”¹ console</h3>

```md
Python/Django Developer
Software Development (All Levels)
Python - Machine Learning SME
Coding teacher for teaching Scratch & Python
Python Developer
Python Developer
Fraud Modeler - Credit Cards / Banking
Logistics Analyst
Online Python Teacher
Python Engineer
Python developer
Python Developer
Analytic Methodologist
Data Scientist with Python
Remote Python Developer
Remote Python Developer
Python Developer Ex Google
Python Developer
3D Solutions Analyst (REMOTE)
Entry Level Software Engineer
Backend Engineer (Python)
Entry Level Python Developer
Data Analyst
Lead Python Developer
Python Developer
Python Developer
Python developer
Data Scientist
AWS Python Developer
Data Analyst: Technical Business Intelligence
USSTRATCOM - Analytic Methodologist
Informatica for Google BigQuery
MACHINE LEARNING DATA SCIENTIST - PYTHON AND R
Python Developer
Database Developer
Basketball Data Scientist (remote opportunity)
Python Developer
Python Developer with AWs
Python Developer
Junior Trader (Remote)
Python Developer
Jr. Software Engineer
Python Developer
Backend software engineer (python)
Python Developer
Software Developer â€“ Entry Level
Python Engineer
Data Scientist
Python Developer
Python Developer
```

## 2.7 Extracting Companies

<h3>ğŸ”¸ indeed.py</h3>

```py
import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

# pagination ì¤‘ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸
def extract_indeed_pages():
  result = requests.get(URL)
  
  soup = BeautifulSoup(result.text, 'html.parser')
  
  # htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
  pagination = soup.find("div", {"class": "pagination"})
  
  links = pagination.find_all('a')
  
  pages = []
  for link in links[:-1]:
    pages.append(int(link.string))
  
  max_page = pages[-1]
  return max_page

# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
def extract_indeed_jobs(last_page):
  jobs = []
  # 1í˜ì´ì§€ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ forë¬¸ ì£¼ì„ ì²˜ë¦¬
  #for page in range(last_page):
  result = requests.get(f"{URL}&start={0*LIMIT}")
  soup = BeautifulSoup(result.text, 'html.parser')
  results = soup.find_all("div", {"class": "job_seen_beacon"})
  for result in results:
    resultContent = result.find("td", {"class": "resultContent"})
    # title
    title = resultContent.find("h2", {"class": "jobTitle"}).find("span", {"class": None})["title"]
    # company
    company = resultContent.find("div", {"class":"company_location"}).find("span", {"class": "companyName"}).string
    print(f"{title} >>> {company}")
  return jobs
```

<h3>ğŸ”¹ console</h3>

```md
Software Development (All Levels) >>> Insurity
Entry Level Software Engineer >>> Avant
Python/Django Developer >>> Delta
Python - Machine Learning SME >>> Envision
Coding teacher for teaching Scratch & Python >>> YoumeCan Education Center
Python Developer >>> Mechlance INC
Python Developer >>> Mobile Mechanic
Fraud Modeler - Credit Cards / Banking >>> Acunor
Logistics Analyst >>> Lululemon
Software Engineer (Early Career) >>> Apple
Python Engineer >>> Highbrow-Tech
Online Python Teacher >>> YC Solutions Pvt. Ltd.
Python Developer >>> Integration Developer Network LLC
Python/Odoo ERP Developer >>> Novobi, LLC
Python developer >>> Vyze, Inc.
Remote Python Developer >>> Piper Companies
Software Engineer I ( 100% Remote) >>> Windstream Communications
Data Scientist with Python >>> Techladder
Analytic Methodologist >>> Constellation West
Remote Python Developer >>> CTI Consulting
Python Developer Ex Google >>> Laiba Technologies
3D Solutions Analyst (REMOTE) >>> Under Armour
Entry Level Python Developer >>> Marlabs
Backend Engineer (Python) >>> Metaverse HQ
Python Developer >>> WorkCog inc
Data Analyst >>> Young Life
MACHINE LEARNING DATA SCIENTIST - PYTHON AND R >>> InspiHER Tech
Lead Python Developer >>> Interaslabs.llc
Python Developer >>> Yaddala Consulting
Data Analyst: Technical Business Intelligence >>> Barstool Sports
USSTRATCOM - Analytic Methodologist >>> Apogee Engineering, LLC
AWS Python Developer >>> Inscope Global Solutions
Data Scientist >>> FIIDUS
Informatica for Google BigQuery >>> Kaygen
Python Developer >>> Business Intelli Solutions
Python Developer >>> AGM Tech Solutions, INC.
Database Developer >>> Integration Developer Network LLC
Python developer >>> Stefanini Group
Python Developer >>> Aquatic Capital Management
Python Developer >>> Santex Group
Python Developer with AWs >>> Innovative BI Solutions Inc
Basketball Data Scientist (remote opportunity) >>> Madison Square Garden Entertainment
Jr. Software Engineer >>> NBCUniversal
Backend software engineer (python) >>> Benchmark IT Solutions
Python Developer >>> Swan Software Solutions
Python Developer >>> Benedsoft
Software Developer â€“ Entry Level >>> Grant Street Group
Python Developer >>> Infinizi IT Solutions Pvt. Ltd.
Python Engineer >>> Techno corporation inc
Python Developer >>> Morgan Stanley
```

## 2.8 Extracting Locations and Finishing up

`id`ê¹Œì§€ ê°€ì ¸ì˜¤ë„ë¡ íŒŒì‹±í•˜ê¸°

`https://www.indeed.com/viewjob?jk={id}` ì™€ ê°™ì´ ë§í¬ ìƒì„± ê°€ëŠ¥!

<h3>ğŸ”¸ main.py</h3>

```py
from indeed import extract_indeed_pages, extract_indeed_jobs

last_indeed_page = extract_indeed_pages()

indeed_jobs = extract_indeed_jobs(last_indeed_page)

print(indeed_jobs)
```

<h3>ğŸ”¸ indeed.py</h3>

```py
import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

# pagination ì¤‘ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸
def extract_indeed_pages():
  result = requests.get(URL)
  
  soup = BeautifulSoup(result.text, 'html.parser')
  
  # htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
  pagination = soup.find("div", {"class": "pagination"})
  
  links = pagination.find_all('a')
  
  pages = []
  for link in links[:-1]:
    pages.append(int(link.string))
  
  max_page = pages[-1]
  return max_page

# íšŒì‚¬ ì •ë³´ íŒŒì‹±
def extract_job(html):
  resultContent = html.find("div", {"class": "job_seen_beacon"}).find("td", {"class": "resultContent"})
  # id
  id = html["data-jk"]
  # title
  title = resultContent.find("h2", {"class": "jobTitle"}).find("span", {"class": None})["title"]
  # company
  company = resultContent.find("div", {"class": "company_location"}).find("span", {"class": "companyName"}).string
  # location
  location = resultContent.find("div", {"class": "companyLocation"}).string
  
  return {'id': id, 'title': title, 'company': company, 'location': location, 'link': f"https://www.indeed.com/viewjob?jk={id}"}

# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
def extract_indeed_jobs(last_page):
  jobs = []
  # ëª¨ë“  í˜ì´ì§€ ì§ì—… ì¡°íšŒ
  for page in range(last_page):
    print(f"Scrapping page {page}")
    result = requests.get(f"{URL}&start={page*LIMIT}")
    soup = BeautifulSoup(result.text, 'html.parser')
    results = soup.find_all("a", {"class": "tapItem"})
    for result in results:
      job = extract_job(result)
      jobs.append(job)
  return jobs
```

<h3>ğŸ”¹ console</h3>

```md
Scrapping page 0
Scrapping page 1
Scrapping page 2
Scrapping page 3
Scrapping page 4
[{'id': 'b6d26975703d41c2', 'title': 'Python - Machine Learning SME', 'company': 'Envision', 'location': 'Remote', 'link': 'https://www.indeed.com/viewjob?jk=b6d26975703d41c2'}, {'id': '5a91a49780ab17df', 'title': 'Sr Data Scientist', 'company': 'Zillow', 'location': 'Remote', 'link': 'https://www.indeed.com/viewjob?jk=5a91a49780ab17df'}, {'id': '2bcd843c58159429', 'title': 'Software Engineer (Early Career)', 'company': 'Apple', 'location': None, 'link': 'https://www.indeed.com/viewjob?jk=2bcd843c58159429'}, {'id': '6d0ab231885eac14', 'title': 'GIS Analyst', 'company': 'Bruce Harris & Associates, Inc', 'location': 'Remote', 'link': 'https://www.indeed.com/viewjob?jk=6d0ab231885eac14'}, {'id': '788dc9dd8ade27a6', 'title': 'Python/Django Developer', 'company': 'Delta', 'location': None, 'link': 'https://www.indeed.com/viewjob?jk=788dc9dd8ade27a6'}, {'id': 'ebcc2ad72eda66fb', 'title': 'QT Software Engineer (Python and C++)', 'company': 'TriSearch', 'location': 'Remote', 'link': 'https://www.indeed.com/viewjob?jk=ebcc2ad72eda66fb'}, {'id': '57654f0e7ccfc3b7', 'title': ' ... ìƒëµ
```

## 2.9 StackOverflow Pages

`https://stackoverflow.com/jobs/companies?q=python` ë¥¼ íŒŒì‹±í•´ë³´ì ! pythonìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” urlì´ë‹¤.

ìš°ì„  stackoverflowì™€ êµ¬ë¶„í•˜ê¸° ìœ„í•´ì„œ `indeed.py`, `so.py` ë¡œ ë‚˜ëˆ ì£¼ì—ˆê³ , `main.py`ì½”ë“œë¥¼ ì¢€ ì •ë¦¬í•˜ì˜€ë‹¤.

`so.py`ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê¸°ì¡´ indeed íŒŒì‹± ë©”ì†Œë“œëŠ” ì£¼ì„ì²˜ë¦¬ í•´ì£¼ì—ˆê³ , `pagination`ì˜ `a` íƒœê·¸ë¥¼ `find_all`ë¡œ ìš°ì„  ê°€ì ¸ì™”ë‹¤.

<h3>ğŸ”¸ main.py</h3>

```py
from indeed import get_jobs as get_indeed_jobs
from so import get_jobs as get_so_jobs

#indeed_jobs = get_indeed_jobs()
so_jobs = get_so_jobs()
```

<h3>ğŸ”¸ indeed.py</h3>

```py
import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

# pagination ì¤‘ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸
def get_last_page():
  result = requests.get(URL)
  
  soup = BeautifulSoup(result.text, 'html.parser')
  
  # htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
  pagination = soup.find("div", {"class": "pagination"})
  
  links = pagination.find_all('a')
  
  pages = []
  for link in links[:-1]:
    pages.append(int(link.string))
  
  max_page = pages[-1]
  return max_page

# íšŒì‚¬ ì •ë³´ íŒŒì‹±
def extract_job(html):
  resultContent = html.find("div", {"class": "job_seen_beacon"}).find("td", {"class": "resultContent"})
  # id
  id = html["data-jk"]
  # title
  title = resultContent.find("h2", {"class": "jobTitle"}).find("span", {"class": None})["title"]
  # company
  company = resultContent.find("div", {"class": "company_location"}).find("span", {"class": "companyName"}).string
  # location
  location = resultContent.find("div", {"class": "companyLocation"}).string
  
  return {'title': title, 'company': company, 'location': location, 'link': f"https://www.indeed.com/viewjob?jk={id}"}

# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
def extract_jobs(last_page):
  jobs = []
  # ëª¨ë“  í˜ì´ì§€ ì§ì—… ì¡°íšŒ
  for page in range(last_page):
    print(f"Scrapping page {page}")
    result = requests.get(f"{URL}&start={page*LIMIT}")
    soup = BeautifulSoup(result.text, 'html.parser')
    results = soup.find_all("a", {"class": "tapItem"})
    for result in results:
      job = extract_job(result)
      jobs.append(job)
  return jobs

def get_jobs():
  last_page = get_last_page()
  jobs = extract_jobs(last_page)
  return jobs
```

<h3>ğŸ”¸ so.py</h3>

```py
import requests
from bs4 import BeautifulSoup

URL =  f"https://stackoverflow.com/jobs/companies?q=python"

def get_last_page():
  result = requests.get(URL)
  soup = BeautifulSoup(result.text, "html.parser")
  pages = soup.find("div", {"class": "s-pagination"}).find_all("a")
  print(pages)
  
def get_jobs():
  last_page = get_last_page()
  return []
```

<h3>ğŸ”¹ console</h3>

```md
[<a class="s-pagination--item is-selected" href="/jobs/companies?q=python" title="page 1 of 21">
<span>1</span>
</a>, <a class="s-pagination--item" href="/jobs/companies?q=python&amp;pg=2" title="page 2 of 21">
<span>2</span>
</a>, <a class="s-pagination--item" href="/jobs/companies?q=python&amp;pg=3" title="page 3 of 21">
<span>3</span>
</a>, <a class="s-pagination--item" href="/jobs/companies?q=python&amp;pg=4" title="page 4 of 21">
<span>4</span>
</a>, <a class="s-pagination--item" href="/jobs/companies?q=python&amp;pg=21" title="page 21 of 21">
<span>21</span>
</a>, <a class="s-pagination--item" href="/jobs/companies?q=python&amp;pg=2" title="page 2 of 21">
<span>next</span><i class="material-icons">chevron_right</i>
</a>]
```

## 2.10 StackOverflow extract jobs

::: tip strip
`get_text(strip=True)` ë¥¼ ì‚¬ìš©í•˜ë©´ textë¥¼ ê°€ì ¸ì˜´ê³¼ ë™ì‹œì— ì•ë’¤ ê³µë°±ì„ ì˜ë¼ì¤€ë‹¤.
[ì°¸ê³ ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text)

```py
markup = '<a href="http://example.com/">\nI linked to <i>example.com</i>\n</a>'
soup = BeautifulSoup(markup, 'html.parser')

soup.get_text()
'\nI linked to example.com\n'
soup.i.get_text()
'example.com'

soup.get_text("|", strip=True)
'I linked to|example.com'
```
:::

<h3>ğŸ”¸ so.py</h3>

```py
import requests
from bs4 import BeautifulSoup

URL =  f"https://stackoverflow.com/jobs/companies?q=python"

# ë§ˆì§€ë§‰ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
def get_last_page():
  result = requests.get(URL)
  soup = BeautifulSoup(result.text, "html.parser")
  pages = soup.find("div", {"class": "s-pagination"}).find_all("a")
  # ë§ˆì§€ë§‰(-1)ì€ nextë²„íŠ¼ì´ë¯€ë¡œ ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ê±°(-2)ê°€ last page
  # strip=Trueë¥¼ í™œìš©í•˜ì—¬ ì•ë’¤ ê³µë°± ìë¥´ê¸°
  last_page = pages[-2].get_text(strip=True) 
  return int(last_page)

# íšŒì‚¬ ê°€ì ¸ì˜¤ê¸°
def extract_companies(last_page):
  companies = []
  # last pageì˜ ê°œìˆ˜ë§Œí¼ ë°°ì—´ ë§Œë“¤ì–´ì„œ forë¬¸ ëŒë¦¬ê¸°
  for page in range(last_page):
    result = requests.get(f"{URL}&pg={page + 1}")
    soup = BeautifulSoup(result.text, "html.parser")
    results = soup.find_all("div", {"class": "-company"})
    for result in results:
      print(result.find("div", {"class": "dismiss-trigger"})["data-id"])
   
def get_jobs():
  last_page = get_last_page()
  companies = extract_companies(last_page)
  return companies
```

<h3>ğŸ”¹ console</h3>

```md
31152
17914
26760
32154
3060
...
ì¤‘ëµ
...
32169
4603
32176
23691
20917
```

## 2.11~12 StackOverflow extract job

íšŒì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì™€ íŒŒì‹±í•´ì£¼ì—ˆë‹¤.

::: tip recursive
`find_all("title", recursive=False)`ë¥¼ ì‚¬ìš©í•˜ë©´ ì²«ë‹¨ê³„ë§Œ ì°¾ê³ , ê·¸ ì•ˆì— ê¹Šìˆ™í•œ íƒœê·¸ëŠ” ì°¾ì§€ ì•ŠëŠ”ë‹¤.
[ì°¸ê³ ](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-recursive-argument)

```py
soup.html.find_all("title")
# [<title>The Dormouse's story</title>]

soup.html.find_all("title", recursive=False)
# []
```
:::

<h3>ğŸ”¸ indeed.py</h3>

```py
import requests
from bs4 import BeautifulSoup

URL =  f"https://stackoverflow.com/jobs/companies?q=python"

# ë§ˆì§€ë§‰ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
def get_last_page():
  result = requests.get(URL)
  soup = BeautifulSoup(result.text, "html.parser")
  pages = soup.find("div", {"class": "s-pagination"}).find_all("a")
  # ë§ˆì§€ë§‰(-1)ì€ nextë²„íŠ¼ì´ë¯€ë¡œ ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ê±°(-2)ê°€ last page
  # strip=Trueë¥¼ í™œìš©í•˜ì—¬ ì•ë’¤ ê³µë°± ìë¥´ê¸°
  last_page = pages[-2].get_text(strip=True) 
  return int(last_page)

def extract_company(html):
  content = html.find("div", {"class": "flex--item fl1 text mb0"})
  # company
  company = content.find("h2").find("a", {"class": "s-link"}).string

  location, industry = content.find_all("div", {"class": "flex--item fc-black-500 fs-body1"})
  # location
  location = location.get_text(strip=True)
  # industry
  industry = industry.get_text(strip=True)
  print(location, industry)
  return {"company": company, "location": location, "industry": industry}
  
  
# íšŒì‚¬ ê°€ì ¸ì˜¤ê¸°
def extract_companies(last_page):
  companies = []
  # last pageì˜ ê°œìˆ˜ë§Œí¼ ë°°ì—´ ë§Œë“¤ì–´ì„œ forë¬¸ ëŒë¦¬ê¸°
  for page in range(last_page):
    result = requests.get(f"{URL}&pg={page + 1}")
    soup = BeautifulSoup(result.text, "html.parser")
    results = soup.find_all("div", {"class": "-company"})
    for result in results:
      company = extract_company(result)
      companies.append(company)
  return companies
  
def get_jobs():
  last_page = get_last_page()
  companies = extract_companies(last_page)
  return companies
```

<h3>ğŸ”¹ console</h3>

```md
Edinburgh; Beirut; Bozeman Cloud Computing, Education Technology, SaaS
Dublin 1 Agile Software Development, Cloud-Based Solutions, Computer Software
MÃ¼nchen Computer Vision, Image Guided Surgery, Medical Imaging
United States Cybersecurity, Healthcare
Elkridge; Linthicum Heights; Vienna Computer Software
...
ì¤‘ëµ
...
No office location Retail, Technical Services, Web Technology
Fulton Business to Business, Security Software
No office location Bioinformatics, Computer Software, Digital Health
No office location Agile Software Development, Software Development / Engineering, Technology Staffing
Berlin Agile Software Development, Automotive
```

## 2.13 StackOverflow Finish

`indeed`ì™€ `stackoverflow` ì—ì„œ íŒŒì‹±í•œ ê²ƒë“¤ì„ í•©ì³ì£¼ì.

<h3>ğŸ”¸ main.py</h3>

```py
from indeed import get_jobs as get_indeed_jobs
from so import get_jobs as get_so_jobs

indeed_jobs = get_indeed_jobs()
so_jobs = get_so_jobs()

jobs = so_jobs + indeed_jobs
```

ê°ê° scrapping ì„±ê³µì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì´ forë¬¸ì•ˆì— `print`í•´ì£¼ì

<h3>ğŸ”¸ indeed.py</h3>

```py
.
.
.
# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
def extract_jobs(last_page):
  jobs = []
  # ëª¨ë“  í˜ì´ì§€ ì§ì—… ì¡°íšŒ
  for page in range(last_page):
    print(f"Scrapping ID: Page: {page}")
.
.
.
```

<h3>ğŸ”¸ so.py</h3>

```py
.
.
.
# íšŒì‚¬ ê°€ì ¸ì˜¤ê¸°
def extract_companies(last_page):
  companies = []
  # last pageì˜ ê°œìˆ˜ë§Œí¼ ë°°ì—´ ë§Œë“¤ì–´ì„œ forë¬¸ ëŒë¦¬ê¸°
  for page in range(last_page):
    print(f"Scrapping SO: Page: {page}")
.
.
.

```

<h3>ğŸ”¹ console</h3>

```md
Scrapping ID: Page: 0
Scrapping ID: Page: 1
Scrapping ID: Page: 2
Scrapping ID: Page: 3
Scrapping ID: Page: 4
Scrapping SO: Page: 0
Scrapping SO: Page: 1
Scrapping SO: Page: 2
Scrapping SO: Page: 3
Scrapping SO: Page: 4
Scrapping SO: Page: 5
Scrapping SO: Page: 6
Scrapping SO: Page: 7
Scrapping SO: Page: 8
Scrapping SO: Page: 9
Scrapping SO: Page: 10
Scrapping SO: Page: 11
Scrapping SO: Page: 12
Scrapping SO: Page: 13
Scrapping SO: Page: 14
Scrapping SO: Page: 15
Scrapping SO: Page: 16
Scrapping SO: Page: 17
Scrapping SO: Page: 18
Scrapping SO: Page: 19
Scrapping SO: Page: 20
```

## 2.14 What is CSV

<h3> CSV : Comma Separated Values</h3>

* vsCode ì—ì„œ `ExcelViewer` í”ŒëŸ¬ê·¸ì¸ì„ ì„¤ì¹˜í•œë‹¤.

![vuepress](../.vuepress/public/img/lecture/01/00.png)

* we.csv íŒŒì¼ì„ ìƒì„±í•œë‹¤.

```csv
name, last Name, age, gender
nico, serrano, 12, male
nico, serrano, 12, male
nico, serrano, 12, male
```

* we.csv íŒŒì¼ì„ vsCodeì—ì„œ previewë¡œ ì—´ì–´ë³¸ë‹¤.

![vuepress](../.vuepress/public/img/lecture/01/01.png)

* google spreadsheetì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ë³¸ë‹¤. 

![vuepress](../.vuepress/public/img/lecture/01/02.png)

ì„ì˜ë¡œ ì•„ë˜ì™€ ê°™ì´ `save.py` íŒŒì¼ ìƒì„±

<h3>ğŸ”¸ main.py</h3>

```py
from indeed import get_jobs as get_indeed_jobs
from so import get_jobs as get_so_jobs
from save import save_to_file

indeed_jobs = get_indeed_jobs()
so_jobs = get_so_jobs()

jobs = so_jobs + indeed_jobs
save_to_file(jobs)
```

<h3>ğŸ”¸ save.py</h3>

```py
import csv

def save_to_file(jobs):
  return 
```

## 2.15 Saving to CSV

<h3>ğŸ”¸ main.py</h3>

``` py
from indeed import get_jobs as get_indeed_jobs
from so import get_jobs as get_so_jobs
from save import save_to_file

indeed_jobs = get_indeed_jobs()
#so_jobs = get_so_jobs()

jobs = indeed_jobs
save_to_file(jobs)
```

<h3>ğŸ”¸ save.py</h3>

```py
import csv

def save_to_file(jobs):
  file = open("jobs.csv", mode="w")
  writer = csv.writer(file)
  # í—¤ë”ì¤„ ìƒì„±
  writer.writerow(["title", "company", "location", "link"])
  for job in jobs:
    # dictì—ì„œ valuesë§Œ ê°€ì ¸ì˜¤ë©´ dict_valuesê°€ typeì„
    # ë”°ë¼ì„œ listë¡œ cast í•´ì¤€ë‹¤
    writer.writerow(list(job.values()))
  return 
```

<h3>ğŸ”¹ jobs.csv</h3>

```csv
title,company,location,link
Remote Python Developer,CTI Consulting,,https://www.indeed.com/viewjob?jk=75422ff0a5cfbe28
Python Developer,Aquatic Capital Management,"Remote in Chicago, IL",https://www.indeed.com/viewjob?jk=5bdb3c2265c60c4a
Senior Python Developer,Gallup,,https://www.indeed.com/viewjob?jk=b3a32bc1a87689e7
C++/Python Developer,FIIDUS,Remote,https://www.indeed.com/viewjob?jk=3720520a9b3f386f
Informatica for Google 
.
.
.
```

## 2.16 OMG THIS IS AWESOME

ë§ˆì§€ë§‰ìœ¼ë¡œ ì½”ë“œ ì •ë¦¬ë¥¼ í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë˜ë©°, csv íŒŒì¼ ë‘ê°œê°€ ìƒì„±ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

 [leehyungwon_python_scrapper_replit](https://replit.com/@HYUNGWONLEE/Python-scrapper)

<h3>ğŸ”¸ main.py</h3>

```py
from indeed import get_jobs as get_indeed_jobs
from so import get_companies as get_so_companies
from save import save_to_file_jobs, save_to_file_companies

indeed_jobs = get_indeed_jobs()
so_companies = get_so_companies()

save_to_file_jobs(indeed_jobs)
save_to_file_companies(so_companies)
```

<h3>ğŸ”¸ indeed.py</h3>

```py
import requests
from bs4 import BeautifulSoup

LIMIT = 50
URL = f"https://www.indeed.com/jobs?q=python&limit={LIMIT}"

# pagination ì¤‘ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸
def get_last_page():
  result = requests.get(URL)
  
  soup = BeautifulSoup(result.text, 'html.parser')
  
  # htmlë‚´ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
  pagination = soup.find("div", {"class": "pagination"})
  
  links = pagination.find_all('a')
  
  pages = []
  for link in links[:-1]:
    pages.append(int(link.string))
  
  max_page = pages[-1]
  return max_page

# íšŒì‚¬ ì •ë³´ íŒŒì‹±
def extract_job(html):
  resultContent = html.find("div", {"class": "job_seen_beacon"}).find("td", {"class": "resultContent"})
  # id
  id = html["data-jk"]
  # title
  title = resultContent.find("h2", {"class": "jobTitle"}).find("span", {"class": None})["title"]
  # company
  company = resultContent.find("div", {"class": "company_location"}).find("span", {"class": "companyName"}).string
  # location
  location = resultContent.find("div", {"class": "companyLocation"}).string
  
  return {'title': title, 'company': company, 'location': location, 'link': f"https://www.indeed.com/viewjob?jk={id}"}

# ê° pageì˜ start index êµ¬í•˜ì—¬ request ë‚ ë ¤ë³´ê¸°
def extract_jobs(last_page):
  jobs = []
  # ëª¨ë“  í˜ì´ì§€ ì§ì—… ì¡°íšŒ
  for page in range(last_page):
    print(f"Scrapping ID: Page: {page}")
    result = requests.get(f"{URL}&start={page*LIMIT}")
    soup = BeautifulSoup(result.text, 'html.parser')
    results = soup.find_all("a", {"class": "tapItem"})
    for result in results:
      job = extract_job(result)
      jobs.append(job)
  return jobs

def get_jobs():
  last_page = get_last_page()
  jobs = extract_jobs(last_page)
  return jobs
```

<h3>ğŸ”¸ so.py</h3>

```py
import requests
from bs4 import BeautifulSoup

URL =  f"https://stackoverflow.com/jobs/companies?q=python"

# ë§ˆì§€ë§‰ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
def get_last_page():
  result = requests.get(URL)
  soup = BeautifulSoup(result.text, "html.parser")
  pages = soup.find("div", {"class": "s-pagination"}).find_all("a")
  # ë§ˆì§€ë§‰(-1)ì€ nextë²„íŠ¼ì´ë¯€ë¡œ ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ê±°(-2)ê°€ last page
  # strip=Trueë¥¼ í™œìš©í•˜ì—¬ ì•ë’¤ ê³µë°± ìë¥´ê¸°
  last_page = pages[-2].get_text(strip=True) 
  return int(last_page)

def extract_company(html):
  content = html.find("div", {"class": "flex--item fl1 text mb0"})
  # company
  company = content.find("h2").find("a", {"class": "s-link"}).string

  location, industry = content.find_all("div", {"class": "flex--item fc-black-500 fs-body1"})
  # location
  location = location.get_text(strip=True)
  # industry
  industry = industry.get_text(strip=True)
  
  # link
  link = content.find("h2").find("a", {"class": "s-link"})['href']

  return {"company": company, "location": location, "industry": industry, "apply_link": f"https://stackoverflow.com{link}"}
  
  
# íšŒì‚¬ ê°€ì ¸ì˜¤ê¸°
def extract_companies(last_page):
  companies = []
  # last pageì˜ ê°œìˆ˜ë§Œí¼ ë°°ì—´ ë§Œë“¤ì–´ì„œ forë¬¸ ëŒë¦¬ê¸°
  for page in range(last_page):
    print(f"Scrapping SO: Page: {page}")
    result = requests.get(f"{URL}&pg={page + 1}")
    soup = BeautifulSoup(result.text, "html.parser")
    results = soup.find_all("div", {"class": "-company"})
    for result in results:
      company = extract_company(result)
      companies.append(company)
  return companies
  
def get_companies():
  last_page = get_last_page()
  companies = extract_companies(last_page)
  return companies
```

<h3>ğŸ”¸ save.py</h3>

```py
import csv

def save_to_file_jobs(jobs):
  file = open("jobs.csv", mode="w")
  writer = csv.writer(file)
  # í—¤ë”ì¤„ ìƒì„±
  writer.writerow(["title", "company", "location", "link"])
  for job in jobs:
    # dictì—ì„œ valuesë§Œ ê°€ì ¸ì˜¤ë©´ dict_valuesê°€ typeì„
    # ë”°ë¼ì„œ listë¡œ cast í•´ì¤€ë‹¤
    writer.writerow(list(job.values()))
  return 
  
def save_to_file_companies(companies):
  file = open("companies.csv", mode="w")
  writer = csv.writer(file)
  # í—¤ë”ì¤„ ìƒì„±
  writer.writerow(["company", "location", "industry", "apply_link"])
  for company in companies:
    # dictì—ì„œ valuesë§Œ ê°€ì ¸ì˜¤ë©´ dict_valuesê°€ typeì„
    # ë”°ë¼ì„œ listë¡œ cast í•´ì¤€ë‹¤
    writer.writerow(list(company.values()))
  return 
```

<h3>ğŸ”¹ console</h3>

```md
Scrapping ID: Page: 0
Scrapping ID: Page: 1
Scrapping ID: Page: 2
Scrapping ID: Page: 3
Scrapping ID: Page: 4
Scrapping SO: Page: 0
Scrapping SO: Page: 1
Scrapping SO: Page: 2
Scrapping SO: Page: 3
Scrapping SO: Page: 4
Scrapping SO: Page: 5
Scrapping SO: Page: 6
Scrapping SO: Page: 7
Scrapping SO: Page: 8
Scrapping SO: Page: 9
Scrapping SO: Page: 10
Scrapping SO: Page: 11
Scrapping SO: Page: 12
Scrapping SO: Page: 13
Scrapping SO: Page: 14
Scrapping SO: Page: 15
Scrapping SO: Page: 16
Scrapping SO: Page: 17
Scrapping SO: Page: 18
Scrapping SO: Page: 19
Scrapping SO: Page: 20
```

<h3>ğŸ”¹ jobs.csv</h3>

```md
title,company,location,link
"Security Engineer- AWS, Python",The Getch,Remote,https://www.indeed.com/viewjob?jk=5fba88b67d1b72dc
Python Developer,Paktolus,Remote,https://www.indeed.com/viewjob?jk=df1cce3cc988f374
Python Developer,Simplified IT Solutions,Remote,https://www.indeed.com/viewjob?jk=7e3a3e84485bb544
Python Developer,EMR CPR LLC,"Austin, TX",https://www.indeed.com/viewjob?jk=52298adb7d458010
Senior Python AWS Developer,DataAxxis,,https://www.indeed.com/viewjob?jk=cd01de5c4adc7ac4
Associate Solutions Architect â€“ Early Career 2022 (US),"Amazon Web Services, Inc.",,https://www.indeed.com/viewjob?jk=3905541e1957ec4a
Python Developer,Oremda Infotech Inc.,Remote,https://www.indeed.com/viewjob?jk=2c04530f755a2932
Senior Software Engineer,University of Nebraska Medical Center,,https://www.indeed.com/viewjob?jk=067a8cf9dccbdc70
...ìƒëµ
```

<h3>ğŸ”¹ companies.csv</h3>

```md
company,location,industry,apply_link
Administrate,Edinburgh; Beirut; Bozeman,"Cloud Computing, Education Technology, SaaS",https://stackoverflow.com/jobs/companies/administrate?c=MYHq0mvrMlWD3iKY&q=python
"Arista Networks, Inc",Dublin 1,"Agile Software Development, Cloud-Based Solutions, Computer Software",https://stackoverflow.com/jobs/companies/www-arista-com?
...ìƒëµ
```

# âš¡ 3. GET READY FOR DJANGO

## 3.0 Django is AWESOME

[Django](https://www.djangoproject.com/) ì†Œê°œ !

## 3.1 *args **kwargs

DjangoëŠ” ë¬´í•œ arguments(*args)ë¥¼ ì¤„ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ key=valueì¸ argumentë¥¼ ì£¼ë ¤ë©´ `**kwargs`ë¥¼ ì¨ì•¼í•œë‹¤. `keyword arguments`ì˜ ì¶•ì•½ì–´ì´ë‹¤.

<h3>ğŸ”¸ py</h3>

```py
def plus(a, b, *args, **kwargs): 
  print(args)
  print(kwargs) # key=valueê°’ì€ keyword argumentë¡œ ë°›ì•„ì•¼í•¨
  return a + b

plus(1, 2, 3, 4, 5, 1, 2, 3, 4, 3, 4, 5, hello=True, bye=True)
```

<h3>ğŸ”¹ console</h3>

```md
(3, 4, 5, 1, 2, 3, 4, 3, 4, 5)
{'hello': True, 'bye': True}
```

ë¬´í•œ ê³„ì‚°ê¸°ë¥¼ ë§Œë“¤ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

<h3>ğŸ”¸ py</h3>

```py
def plus(*args):
  result = 0
  for number in args:
    result += number
  print(result)

plus(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
```

<h3>ğŸ”¹ console</h3>

```md
55
```

## 3.2 Intro to Object Oriented Programming

<h3>ğŸ”¸ py</h3>

```py
# ì²­ì‚¬ì§„ (blueprint)
class Car():
  wheels = 4
  doors = 4
  windows = 4
  seats = 4

porche = Car() #instance
porche.color = "Red"
print(porche.windows, porche.color)

ferrari = Car()
ferrari.color = "Yellow"
print(ferrari.windows, ferrari.color)

mini = Car()
mini.color = "White"
```

<h3>ğŸ”¹ console</h3>

```md
4 Red
4 Yellow
```

## 3.3 Methods part One

::: tip method
class ì•ˆì— ìˆëŠ” functionì„ methodë¼ê³  í•œë‹¤.
ì „ì—­ìœ¼ë¡œ ì„ ì–¸ëì„ ê²½ìš°ì—ëŠ” function / í´ë˜ìŠ¤ ì•ˆì— ì„ ì–¸ëœ ê²½ìš°ì—ëŠ” method

pythonì€ ëª¨ë“  methodì— 1ê°œì˜ argumentë¥¼ í•„ìˆ˜ë¡œ ê°–ëŠ”ë‹¤.
`method`ì˜ ì²«ë²ˆì§¸ argumentëŠ” methodë¥¼ í˜¸ì¶œí•˜ëŠ” ìê¸° ìì‹ , instanceì´ë‹¤.
:::

<h3>ğŸ”¸ py</h3>

```py
# ì²­ì‚¬ì§„ (blueprint)
class Car():
  wheels = 4
  doors = 4
  windows = 4
  seats = 4
  # method (í´ë˜ìŠ¤ ì•ˆì— ìˆìœ¼ë©´ method/ë°–ì´ë©´ function)
  def start(self):
    print(self.doors)
    print(self.color)
    print("I started")
    
porche = Car()
porche.color = "RED"
porche.start()
```

<h3>ğŸ”¹ console</h3>

```md
4
RED
I started
```

## 3.4 Methods part Two

<h3>ğŸ”¸ py</h3>

```py
# ì²­ì‚¬ì§„ (blueprint)
class Car():
  # initìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê²Œ ë°”ëŒì§í•¨
  def __init__(self, **kwargs):
    # print(kwargs)
    self.wheels = 4
    self.doors = 4
    self.windows = 4
    self.seats = 4
    # ê°’ì´ ì—†ì„ ê²½ìš°, ë‘ë²ˆì§¸ ì¸ì í• ë‹¹
    self.color = kwargs.get("color", "black")
    self.price = kwargs.get("price", "$20")
    
  # method override
  def __str__(self):
    return f"Car with {self.wheels} wheels"

# dir í´ë˜ìŠ¤ ì•ˆ ëª¨ë“ ê²ƒë“¤ì€ listë¡œ ë³´ì—¬ì¤€ë‹¤.
# print(dir(Car))

porche = Car(color="green", price="$40")
# porcheë¥¼ í˜¸ì¶œí• ë•Œë§ˆë‹¤ ê¸°ë³¸ ë©”ì†Œë“œì¸ __str__ì„ í˜¸ì¶œ
print(porche)
print(porche.color, porche.price)

mini = Car()
print(mini.color, mini.price)
```

<h3>ğŸ”¹ console</h3>

```md
Car with 4 wheels
green $40
black $20
```

## 3.5 Extending Classes

<h3>ğŸ”¸ py</h3>

```py
# ì²­ì‚¬ì§„ (blueprint)
class Car():
  # initìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê²Œ ë°”ëŒì§í•¨
  def __init__(self, **kwargs):
    # print(kwargs)
    self.wheels = 4
    self.doors = 4
    self.windows = 4
    self.seats = 4
    # ê°’ì´ ì—†ì„ ê²½ìš°, ë‘ë²ˆì§¸ ì¸ì í• ë‹¹
    self.color = kwargs.get("color", "black")
    self.price = kwargs.get("price", "$20")
    
  # method override
  def __str__(self):
    return f"Car with {self.wheels} wheels"

# extends Car class
class Convertible(Car):
  # ë¶€ëª¨ initì— ì¶”ê°€ ì‘ì—…
  def __init__(self, **kwargs):
    super().__init__(**kwargs) # ë¶€ëª¨ í´ë˜ìŠ¤ í˜¸ì¶œ
    self.time = kwargs.get("time", 10)
  # add method
  def take_off(self):
    return "taking off"
  # override
  def __str__(self):
    return f"Car with no roof"
    
porche = Convertible(color="green", price="$40")

mini = Car()
print(porche)
print(porche.color)
print(porche.take_off())
```

<h3>ğŸ”¹ console</h3>

```md
Car with no roof
green
taking off
```

## 3.6 Whats Next

<h3>ë !</h3>

## .. ğŸ¤ progress

# âš¡ 4. 2020 BONUS CLASS

## 4.0 Welcome to 2020 Update

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.1 Introduction to Flask

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.2 Dynamic URLs and Templates

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.3 Forms and Query Arguments

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.4 Scrapper Integration

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.5 Faster Scrapper

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.6 Rendering Jobs!

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.7 Export Route

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.8 File Download

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.9 Recap

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## 4.10 Conclusions

<h3>ğŸ”¸ py</h3>

<h3>ğŸ”¹ console</h3>

## Reference

[Pythoneìœ¼ë¡œ ì›¹ ìŠ¤í¬ë˜í¼ ë§Œë“¤ê¸°](https://nomadcoders.co/python-for-beginners/lobby)
[python-scrapper-replit](https://replit.com/@HYUNGWONLEE/Python-scrapper)
[Python library](https://docs.python.org/3/library/index.html)